from __future__ import annotations

from dataclasses import dataclass
from typing import Optional, Union, Tuple, Dict, Iterable

import logging
import heapq
from datetime import datetime, timezone
import networkx as nx
import numpy as np
from scipy.spatial import cKDTree
import trimesh as tm
import plotly.graph_objects as go

from .mcf import mean_curvature_flow, MCFResult
from .medial import compute_voronoi_poles

logger = logging.getLogger(__name__)
logger.addHandler(logging.NullHandler())

@dataclass
class Skeleton:
    nodes: np.ndarray  # (k,3)
    edges: np.ndarray  # (e,2) int indices into nodes (from pruned graph, may have cycles)
    graph: nx.Graph    # pruned kNN graph, may contain cycles

    def write_swc(
        self,
        filepath: str,
        *,
        node_type: int = 0,
        default_radius: float = 1.0,
        break_cycles: str = "mst",
        annotate: bool = True,
    ) -> None:
        """Write the skeleton to an SWC file.

        SWC rows: n T x y z R P
        - n: 1-based node id
        - T: node type (default 0=undefined)
        - x y z: coordinates
        - R: radius (use default unless application provides per-node radii)
        - P: parent id (-1 for root)

        If the skeleton graph has cycles, we build a spanning forest (default MST)
        and annotate in the header which edges were removed to break cycles.
        """
        G = self.graph.copy()
        # Ensure positions exist for all nodes
        for n in G.nodes:
            if "pos" not in G.nodes[n]:
                G.nodes[n]["pos"] = np.asarray(self.nodes[n], dtype=float)

        # Spanning forest and list of removed edges
        if break_cycles == "mst":
            forest, removed = _spanning_forest_by_mst(G)
        elif break_cycles == "bfs":
            forest, removed = _spanning_forest_by_bfs(G)
        else:
            # default fallback
            forest, removed = _spanning_forest_by_mst(G)

        # Build SWC order per connected component so parents precede children
        order, parent_of = _swc_bfs_order(forest)

        # Write file
        with open(filepath, "w", encoding="utf-8") as f:
            # Header
            ts = datetime.now(timezone.utc).isoformat()
            f.write(f"# SWC generated by pymcfs on {ts}\n")
            if annotate and removed:
                f.write(f"# cycles_broken: {len(removed)}\n")
                for (u, v) in removed:
                    f.write(f"# removed_edge {u} {v}\n")
            f.write("# Columns: n T x y z R P\n")

            # Map original node to SWC id
            swc_id: dict[int, int] = {}
            next_id = 1
            for n in order:
                swc_id[n] = next_id
                next_id += 1

            # Emit rows
            for n in order:
                x, y, z = map(float, G.nodes[n]["pos"])  # type: ignore[index]
                pid = parent_of.get(n, None)
                p_swc = -1 if pid is None else swc_id[pid]
                f.write(f"{swc_id[n]} {int(node_type)} {x:.6f} {y:.6f} {z:.6f} {float(default_radius):.6f} {p_swc}\n")


    def plot_3d(
        self,
        mesh: tm.Trimesh | tuple[np.ndarray, np.ndarray] | None = None,
        *,
        show_nodes: bool = False,
        node_size: float = 4.0,
        node_color: str = "#d62728",
        edge_color: str = "#1f77b4",
        edge_width: float = 4.0,
        mesh_color: str = "#AAAAAA",
        mesh_opacity: float = 0.3,
        title: str | None = None,
        autoshow: bool = True,
    ) -> "go.Figure":
        """Interactive 3D visualization of the skeleton with optional mesh overlay.

        Parameters
        ----------
        mesh : trimesh.Trimesh or (V,F) or None
            If provided, overlays the original mesh using Plotly Mesh3d. You can pass
            either a trimesh.Trimesh instance or a tuple (vertices, faces).
        show_nodes : bool
            If True, also draw node markers in addition to skeleton edges.
        node_size : float
            Marker size for nodes when show_nodes=True.
        node_color : str
            Color for node markers.
        edge_color : str
            Color for skeleton edges.
        edge_width : float
            Line width for skeleton edges.
        mesh_color : str
            Color for mesh surface.
        mesh_opacity : float
            Opacity for mesh surface in [0,1].
        title : str or None
            Figure title.
        autoshow : bool
            If True, call fig.show() before returning (useful in notebooks).

        Returns
        -------
        plotly.graph_objects.Figure
            The created Plotly figure.
        """
        traces: list[go.BaseTraceType] = []

        # Optional mesh overlay
        if mesh is not None:
            if isinstance(mesh, tm.Trimesh):
                V = np.asarray(mesh.vertices, dtype=float)
                F = np.asarray(mesh.faces, dtype=int)
            else:
                try:
                    V, F = mesh  # type: ignore[misc]
                    V = np.asarray(V, dtype=float)
                    F = np.asarray(F, dtype=int)
                except Exception:
                    raise TypeError("mesh must be a trimesh.Trimesh or a (V,F) tuple")
            if V.size > 0 and F.size > 0:
                mesh_trace = go.Mesh3d(
                    x=V[:, 0], y=V[:, 1], z=V[:, 2],
                    i=F[:, 0], j=F[:, 1], k=F[:, 2],
                    color=mesh_color,
                    opacity=float(np.clip(mesh_opacity, 0.0, 1.0)),
                    name="mesh",
                    flatshading=True,
                    lighting=dict(ambient=0.6, diffuse=0.7, roughness=0.9),
                    showscale=False,
                )
                traces.append(mesh_trace)

        # Skeleton edges as line segments
        P = np.asarray(self.nodes, dtype=float)
        E = np.asarray(self.edges, dtype=int)
        if P.size > 0 and E.size > 0:
            xs: list[float | None] = []
            ys: list[float | None] = []
            zs: list[float | None] = []
            for (a, b) in E:
                pa = P[int(a)]
                pb = P[int(b)]
                xs.extend([float(pa[0]), float(pb[0]), None])
                ys.extend([float(pa[1]), float(pb[1]), None])
                zs.extend([float(pa[2]), float(pb[2]), None])
            edge_trace = go.Scatter3d(
                x=xs, y=ys, z=zs,
                mode="lines",
                line=dict(color=edge_color, width=float(edge_width)),
                name="skeleton",
            )
            traces.append(edge_trace)

        # Optional node markers
        if show_nodes and P.size > 0:
            node_trace = go.Scatter3d(
                x=P[:, 0], y=P[:, 1], z=P[:, 2],
                mode="markers",
                marker=dict(size=float(node_size), color=node_color),
                name="nodes",
            )
            traces.append(node_trace)

        fig = go.Figure(data=traces)
        fig.update_layout(
            title=title or "Skeleton 3D",
            scene=dict(
                xaxis=dict(visible=True),
                yaxis=dict(visible=True),
                zaxis=dict(visible=True),
                aspectmode="data",
            ),
            legend=dict(orientation="h", yanchor="bottom", y=1.02),
            margin=dict(l=0, r=0, t=40, b=0),
        )

        if autoshow:
            fig.show()
        return fig



def _spanning_forest_by_mst(G: nx.Graph) -> tuple[nx.Graph, list[tuple[int, int]]]:
    H = nx.Graph()
    H.add_nodes_from(G.nodes(data=True))
    removed: list[tuple[int, int]] = []
    for comp in nx.connected_components(G):
        sub = G.subgraph(comp).copy()
        # Ensure weights on edges
        for u, v, d in sub.edges(data=True):
            if "weight" not in d:
                try:
                    pu = np.asarray(sub.nodes[u]["pos"], dtype=float)
                    pv = np.asarray(sub.nodes[v]["pos"], dtype=float)
                    d["weight"] = float(np.linalg.norm(pu - pv))
                except Exception:
                    d["weight"] = 1.0
        if sub.number_of_edges() == 0:
            H = nx.compose(H, sub)
            continue
        T = nx.minimum_spanning_tree(sub, weight="weight")
        H = nx.compose(H, T)
        # Edges removed to break cycles
        for (u, v) in sub.edges():
            if not T.has_edge(u, v):
                removed.append((u, v))
    return H, removed


def _spanning_forest_by_bfs(G: nx.Graph) -> tuple[nx.Graph, list[tuple[int, int]]]:
    H = nx.Graph()
    H.add_nodes_from(G.nodes(data=True))
    removed: list[tuple[int, int]] = []
    for comp in nx.connected_components(G):
        nodes = list(comp)
        if not nodes:
            continue
        root = nodes[0]
        Tdir = nx.bfs_tree(G.subgraph(comp), source=root)
        T = nx.Graph()
        T.add_nodes_from(Tdir.nodes(data=True))
        T.add_edges_from(Tdir.edges())
        H = nx.compose(H, T)
        # removed
        sub = G.subgraph(comp)
        for e in sub.edges():
            if not T.has_edge(*e):
                removed.append(e)
    return H, removed


def _swc_bfs_order(G: nx.Graph) -> tuple[list[int], dict[int, int | None]]:
    order: list[int] = []
    parent: dict[int, int | None] = {}
    visited: set[int] = set()
    for comp in nx.connected_components(G):
        comp_nodes = list(comp)
        if not comp_nodes:
            continue
        # Prefer a degree-1 leaf as root if available
        leaves = [n for n in comp_nodes if G.degree(n) <= 1]
        root = leaves[0] if leaves else comp_nodes[0]
        queue = [root]
        visited.add(root)
        parent[root] = None
        while queue:
            u = queue.pop(0)
            order.append(u)
            for v in G.neighbors(u):
                if v in visited:
                    continue
                visited.add(v)
                parent[v] = u
                queue.append(v)
    return order, parent


def _map_targets_to_points(points: np.ndarray, base_points: np.ndarray, base_targets: np.ndarray) -> np.ndarray:
    if len(points) == 0:
        return np.zeros((0, 3), dtype=float)
    tree = cKDTree(base_points)
    d, idx = tree.query(points, k=1)
    return np.asarray(base_targets[idx], dtype=float)


def skeletonize(
    mesh: Union[tm.Trimesh, object],
    *,
    mcf_dt: float = 2e-2,
    mcf_iters: int = 50,
    laplacian_type: str = "cotangent",
    guidance_type: str | None = None,
    guidance_weight: float = 0.0,
    build_graph: str = "mesh",
    knn: int = 12,
    length_quantile: float = 0.7,
    collapse_passes: int = 3,
    collapse_percentile: float = 0.3,
    preserve_branch_degree: int = 3,
    collapse_mode: str = "percentile",
    collapse_ratio: float = 0.1,
    collapse_domain: str = "graph",
    compress_chains: bool = False,
    medial_protect: bool = False,
    medial_protect_threshold: float = 0.5,
    resample_spacing: float | None = None,
    graph_collapse_mode: str = "percentile",
    graph_collapse_ratio: float = 0.1,
    closest_pole_policy: bool = False,
    closest_pole_tol: float = 1.05,
    verbose: bool = False,
    log: Optional[logging.Logger] = None,
) -> Skeleton:
    """Skelcollapse-inspired skeletonization.

    Pipeline (graph-space analogue of Skelcollapse):
    1) Contract the surface by implicit mean curvature flow (MCF).
    2) Use contracted vertices as candidates, optionally voxel-downsample.
    3) Build a kNN graph in Euclidean space on candidate nodes.
    4) Priority edge-collapse: iteratively merge shortest edges while preserving
       junctions (avoid collapsing high-degree nodes). Update node positions to
       midpoints. Repeat for several passes with adaptive thresholds.
    5) Prune short leaf stubs by length quantile; allow cycles to remain.
    6) Optionally compress degree-2 chains into single edges between junctions.

    Returns a `Skeleton` with node array, edge index array, and the NetworkX graph.

    Parameters
    ----------
    mcf_dt, mcf_iters : control MCF contraction speed and iterations.
    laplacian_type : "cotangent" | "mean_value".
    guidance_type : optional soft guidance for MCF (e.g., "centroid").
    guidance_weight : nonnegative weight for guidance term.
    knn : neighbors for initial graph; later passes may rebuild with ~knn//2.
    length_quantile : pruning threshold for leaf stubs.
    collapse_passes : number of simplification passes.
    collapse_percentile : edges below this length percentile are candidates for collapse (when collapse_mode="percentile").
    collapse_mode : "percentile" (default) or "pq" (priority-order without percentile cutoff).
    collapse_ratio : when collapse_mode="pq", collapse up to this fraction of shortest edges per pass (0..1).
    preserve_branch_degree : skip collapsing edges incident to nodes with degree >= this value.
    collapse_domain : "graph" (default) to run graph-space collapse, or "mesh_only" to skip graph collapse and operate only on the mesh-derived graph (closer to Skelcollapse).
    medial_protect : if True and guidance_type=="voronoi", reduce collapses near medial targets based on pole weights.
    medial_protect_threshold : [0..1], edges with both endpoints above this per-vertex weight are less likely to collapse.
    graph_collapse_mode : "percentile" (default) or "pq"; controls graph-space collapse selection.
    graph_collapse_ratio : when graph_collapse_mode="pq", collapse up to this fraction per pass.
    closest_pole_policy : if True and guidance_type=="voronoi", prefer collapses that move nodes closer to their pole targets (skip merges that move away by too much).
    closest_pole_tol : tolerance factor (>1) allowing small increases when evaluating distance to pole targets (1.05 means up to 5% increase allowed).
    """
    _log = log or logger
    # Accept either a Trimesh or a MeshManager
    if isinstance(mesh, tm.Trimesh):
        m = mesh
    else:
        try:
            from .mesh import MeshManager  # type: ignore
        except Exception:
            MeshManager = None  # type: ignore
        if MeshManager is not None and isinstance(mesh, MeshManager):  # type: ignore
            m = mesh.to_trimesh()  # type: ignore[assignment]
        else:
            raise TypeError("mesh must be a trimesh.Trimesh or MeshManager")

    # 1) Contract with MCF
    if verbose:
        _log.info("Skeleton: MCF contraction (dt=%.3g, iters=%d)", mcf_dt, mcf_iters)
    # Optional Voronoi medial guidance
    g_targets = None
    g_diag = None
    g_type = guidance_type
    g_weight = guidance_weight
    pole_targets = None
    if guidance_type == "voronoi" and guidance_weight > 0.0:
        if verbose:
            _log.info("Skeleton: computing Voronoi-pole medial targets")
        targets, weights = compute_voronoi_poles(m)
        # Scale weights by guidance_weight; they are already normalized to [0,1]
        g_targets = targets
        g_diag = weights * float(guidance_weight)
        g_type = None  # disable centroid guidance path
        pole_targets = targets

    mcf_res: MCFResult = mean_curvature_flow(
        m,
        dt=mcf_dt,
        iterations=mcf_iters,
        laplacian_type=laplacian_type,
        guidance_type=g_type,
        guidance_weight=g_weight,
        guidance_targets=g_targets,
        guidance_diag=g_diag,
        record_history=False,
        verbose=verbose,
        log=_log,
    )
    X = mcf_res.vertices

    # 2) Build initial graph
    build_graph = (build_graph or "mesh").lower()
    if build_graph not in ("mesh", "knn"):
        raise ValueError("build_graph must be 'mesh' or 'knn'")

    if build_graph == "mesh":
        # Use all contracted vertices; build graph from mesh faces
        # Optionally compute medial protection weights from Voronoi guidance
        protect_w = None
        protect_thr = None
        if medial_protect and guidance_type == "voronoi":
            try:
                _, weights = compute_voronoi_poles(m)
                protect_w = weights  # already in [0,1]
                protect_thr = float(np.clip(medial_protect_threshold, 0.0, 1.0))
            except Exception:
                protect_w = None
                protect_thr = None

        V_thin, F_thin = _mesh_edge_collapse_thinning(
            X,
            np.asarray(m.faces, dtype=int),
            passes=max(1, collapse_passes // 2),
            length_percentile=collapse_percentile,
            preserve_branch_degree=preserve_branch_degree,
            mode=collapse_mode,
            ratio=collapse_ratio,
            protect_weights=protect_w,
            protect_threshold=protect_thr,
            protect_targets=pole_targets if closest_pole_policy else None,
            closest_pole_policy=closest_pole_policy,
            closest_pole_tol=closest_pole_tol,
            _log=_log if verbose else None,
        )
        nodes = V_thin
        G = _mesh_edge_graph(nodes, F_thin)
        if verbose:
            _log.info("Skeleton: mesh-edge graph built (nodes=%d, edges=%d)", G.number_of_nodes(), G.number_of_edges())
    else:
        # knn: downsample and build kNN graph
        vox = _auto_voxel_size(m)
        nodes = _voxel_downsample(X, voxel_size=vox)
        if verbose:
            _log.info("Skeleton: node candidates after voxel downsample: %d (voxel=%.3g)", nodes.shape[0], vox)
        G = _knn_graph(nodes, k=max(3, knn))
        if verbose:
            _log.info("Skeleton: initial kNN graph built (k=%d, nodes=%d, edges=%d)", knn, G.number_of_nodes(), G.number_of_edges())

    # 4) Optional priority edge-collapse in graph space
    if (collapse_domain or "graph").lower() == "graph":
        # If medial protection requested with Voronoi guidance, map weights to current nodes
        node_protect_w = None
        node_targets = None
        if medial_protect and guidance_type == "voronoi":
            try:
                if protect_w is None:
                    # compute from original mesh if not computed yet
                    _, pw = compute_voronoi_poles(m)
                else:
                    pw = protect_w
                pts = np.array([G.nodes[i]["pos"] for i in G.nodes])
                node_protect_w = _map_weights_to_points(pts, np.asarray(m.vertices), pw)
                protect_thr_val = float(np.clip(medial_protect_threshold, 0.0, 1.0))
            except Exception:
                node_protect_w = None
                protect_thr_val = None
        else:
            protect_thr_val = None

        # If closest-pole policy requested, map pole targets as well
        if closest_pole_policy and guidance_type == "voronoi":
            try:
                if pole_targets is None:
                    pt, _ = compute_voronoi_poles(m)
                else:
                    pt = pole_targets
                pts = np.array([G.nodes[i]["pos"] for i in G.nodes])
                node_targets = _map_targets_to_points(pts, np.asarray(m.vertices), np.asarray(pt))
            except Exception:
                node_targets = None

        # Multiple passes with decreasing thresholds to progressively simplify
        for p in range(max(1, collapse_passes)):
            _graph_priority_edge_collapse(
                G,
                preserve_branch_degree=preserve_branch_degree,
                length_percentile=collapse_percentile,
                mode=graph_collapse_mode,
                ratio=graph_collapse_ratio,
                protect_weights=node_protect_w,
                protect_threshold=protect_thr_val,
                targets=node_targets,
                closest_pole_policy=closest_pole_policy,
                closest_pole_tol=closest_pole_tol,
                verbose=verbose,
                _log=_log,
            )
            # Reconnect with a fresh kNN to ensure local connectivity post merges
            pts = np.array([G.nodes[i]["pos"] for i in G.nodes])
            # Reindex nodes from 0..N-1 for stability
            mapping = {n: idx for idx, n in enumerate(G.nodes)}
            G = nx.relabel_nodes(G, mapping, copy=True)
            for n, pos in enumerate(pts):
                G.nodes[n]["pos"] = pos
            # Rebuild edges based on geometry (helps prevent long cross-links)
            # Use kNN rebuild for both modes to maintain connectivity after merges
            G = _rebuild_knn_graph_from_existing_nodes(G, k=max(3, knn // 2))
            if verbose:
                _log.info("Skeleton: after collapse pass %d, nodes=%d, edges=%d", p + 1, G.number_of_nodes(), G.number_of_edges())

    # 5) Prune leaves by length quantile
    _prune_short_leaves(G, quantile=length_quantile)
    if verbose:
        _log.info("Skeleton: after pruning (q=%.2f), edges=%d", length_quantile, G.number_of_edges())

    # 6) Optionally compress degree-2 chains
    if compress_chains:
        G = _compress_degree_two_chains(G)
        if verbose:
            _log.info("Skeleton: after chain compression, nodes=%d, edges=%d", G.number_of_nodes(), G.number_of_edges())

    # 7) Optional resampling of edges to approx. uniform spacing
    if resample_spacing is not None and resample_spacing > 0:
        G = _resample_edges_uniform(G, float(resample_spacing))
        if verbose:
            _log.info("Skeleton: after resample (h=%.3g), nodes=%d, edges=%d", resample_spacing, G.number_of_nodes(), G.number_of_edges())

    # Pack arrays
    node_index = {n: i for i, n in enumerate(G.nodes)}
    nodes_arr = np.array([G.nodes[n]["pos"] for n in G.nodes], dtype=float)
    edges_arr = np.array([[node_index[u], node_index[v]] for u, v in G.edges], dtype=int)

    return Skeleton(nodes=nodes_arr, edges=edges_arr, graph=G)


def thin_mesh(
    mesh: Union[tm.Trimesh, object],
    *,
    mcf_dt: float = 2e-2,
    mcf_iters: int = 50,
    laplacian_type: str = "cotangent",
    guidance_type: str | None = None,
    guidance_weight: float = 0.0,
    collapse_passes: int = 2,
    collapse_percentile: float = 0.3,
    preserve_branch_degree: int = 3,
    collapse_mode: str = "percentile",
    collapse_ratio: float = 0.1,
    medial_protect: bool = False,
    medial_protect_threshold: float = 0.5,
    verbose: bool = False,
    log: Optional[logging.Logger] = None,
) -> tuple[np.ndarray, np.ndarray]:
    """Contract the mesh via MCF and perform surface-based thinning by edge collapses.

    Returns (V_thin, F_thin) for use in downstream skeletonization or analysis.
    """
    _log = log or logger
    # Normalize mesh input
    if isinstance(mesh, tm.Trimesh):
        m = mesh
    else:
        try:
            from .mesh import MeshManager  # type: ignore
        except Exception:
            MeshManager = None  # type: ignore
        if MeshManager is not None and isinstance(mesh, MeshManager):  # type: ignore
            m = mesh.to_trimesh()  # type: ignore[assignment]
        else:
            raise TypeError("mesh must be a trimesh.Trimesh or MeshManager")

    # Optional Voronoi medial guidance
    g_targets = None
    g_diag = None
    g_type = guidance_type
    g_weight = guidance_weight
    protect_w = None
    protect_thr = None
    if guidance_type == "voronoi" and guidance_weight > 0.0:
        if verbose:
            _log.info("ThinMesh: computing Voronoi medial targets")
        targets, weights = compute_voronoi_poles(m)
        g_targets = targets
        g_diag = weights * float(guidance_weight)
        g_type = None
        if medial_protect:
            protect_w = weights
            protect_thr = float(np.clip(medial_protect_threshold, 0.0, 1.0))

    # MCF contraction
    if verbose:
        _log.info("ThinMesh: MCF contraction (dt=%.3g, iters=%d)", mcf_dt, mcf_iters)
    mcf_res: MCFResult = mean_curvature_flow(
        m,
        dt=mcf_dt,
        iterations=mcf_iters,
        laplacian_type=laplacian_type,
        guidance_type=g_type,
        guidance_weight=g_weight,
        guidance_targets=g_targets,
        guidance_diag=g_diag,
        record_history=False,
        verbose=verbose,
        log=_log,
    )
    X = mcf_res.vertices

    # Mesh-edge collapse thinning
    V_thin, F_thin = _mesh_edge_collapse_thinning(
        X,
        np.asarray(m.faces, dtype=int),
        passes=max(1, collapse_passes),
        length_percentile=collapse_percentile,
        preserve_branch_degree=preserve_branch_degree,
        mode=collapse_mode,
        ratio=collapse_ratio,
        protect_weights=protect_w,
        protect_threshold=protect_thr,
        _log=_log if verbose else None,
    )

    return V_thin, F_thin

def curve_skeleton_from_mesh(
    V: np.ndarray,
    F: np.ndarray,
    *,
    compress_chains: bool = True,
    resample_spacing: float | None = None,
) -> Skeleton:
    """Convert a triangle mesh surface to a curve graph skeleton.

    Steps:
    1) Build a mesh-edge graph from (V, F).
    2) Optionally compress degree-2 chains to simplify.
    3) Optionally resample edges to an approximate uniform spacing.
    """
    if V.size == 0 or F.size == 0:
        G = nx.Graph()
        return Skeleton(nodes=np.zeros((0, 3), dtype=float), edges=np.zeros((0, 2), dtype=int), graph=G)

    G = _mesh_edge_graph(V, F.astype(int, copy=False))

    if compress_chains:
        G = _compress_degree_two_chains(G)

    if resample_spacing is not None and resample_spacing > 0:
        G = _resample_edges_uniform(G, float(resample_spacing))

    # Pack arrays
    node_index = {n: i for i, n in enumerate(G.nodes)}
    nodes_arr = np.array([G.nodes[n]["pos"] for n in G.nodes], dtype=float)
    edges_arr = np.array([[node_index[u], node_index[v]] for u, v in G.edges], dtype=int)

    return Skeleton(nodes=nodes_arr, edges=edges_arr, graph=G)

def _auto_voxel_size(mesh: tm.Trimesh) -> float:
    # Choose voxel size as a small fraction of bbox diagonal
    bb = mesh.bounding_box.extents
    diag = float(np.linalg.norm(bb))
    return max(diag * 0.01, 1e-4)


def _voxel_downsample(points: np.ndarray, voxel_size: float) -> np.ndarray:
    if len(points) == 0:
        return points
    # Compute voxel indices
    mins = points.min(axis=0)
    idx = np.floor((points - mins) / voxel_size).astype(np.int64)
    # Hash to unique voxels
    keys, inverse = np.unique(idx, axis=0, return_inverse=True)
    # Average points in each voxel
    out = np.zeros((len(keys), 3), dtype=float)
    counts = np.bincount(inverse)
    for d in range(3):
        out[:, d] = np.bincount(inverse, weights=points[:, d]) / counts
    return out


def _knn_graph(points: np.ndarray, k: int) -> nx.Graph:
    tree = cKDTree(points)
    G = nx.Graph()
    for i, p in enumerate(points):
        G.add_node(i, pos=p)
    # Query k+1 because first neighbor is the point itself
    dists, nbrs = tree.query(points, k=min(k + 1, len(points)))
    for i in range(len(points)):
        for j, d in zip(nbrs[i], dists[i]):
            if j == i or j < 0:
                continue
            w = float(d)
            if G.has_edge(i, j):
                # keep smallest weight if duplicate
                if w < G[i][j]["weight"]:
                    G[i][j]["weight"] = w
            else:
                G.add_edge(i, j, weight=w)
    return G


def _mesh_edge_graph(points: np.ndarray, faces: np.ndarray) -> nx.Graph:
    """Build an undirected graph from triangle mesh faces.

    Nodes correspond to vertex indices; edges exist for each unique mesh edge
    with weight equal to Euclidean edge length between incident vertices.
    """
    G = nx.Graph()
    for i, p in enumerate(points):
        G.add_node(i, pos=np.asarray(p, dtype=float))
    if faces.size == 0:
        return G
    # Unique edges
    edges = set()
    f = faces.astype(int, copy=False)
    for tri in f:
        i0, i1, i2 = int(tri[0]), int(tri[1]), int(tri[2])
        for (a, b) in ((i0, i1), (i1, i2), (i2, i0)):
            if a == b:
                continue
            e = (a, b) if a < b else (b, a)
            edges.add(e)
    # Add with weights
    P = points
    for (a, b) in edges:
        w = float(np.linalg.norm(P[a] - P[b]))
        G.add_edge(a, b, weight=w)
    return G


def _mesh_unique_edges(faces: np.ndarray) -> np.ndarray:
    """Return unique undirected edges from triangle faces as (E,2) int array sorted within each pair."""
    if faces.size == 0:
        return np.zeros((0, 2), dtype=int)
    e = set()
    for tri in faces:
        i0, i1, i2 = int(tri[0]), int(tri[1]), int(tri[2])
        for (a, b) in ((i0, i1), (i1, i2), (i2, i0)):
            if a == b:
                continue
            if a > b:
                a, b = b, a
            e.add((a, b))
    if not e:
        return np.zeros((0, 2), dtype=int)
    arr = np.fromiter((i for ab in e for i in ab), dtype=int)
    return arr.reshape(-1, 2)


def _vertex_degrees_from_faces(nv: int, faces: np.ndarray) -> np.ndarray:
    """Approximate per-vertex degree from unique mesh edges."""
    deg = np.zeros(nv, dtype=int)
    edges = _mesh_unique_edges(faces)
    if edges.size > 0:
        np.add.at(deg, edges[:, 0], 1)
        np.add.at(deg, edges[:, 1], 1)
    return deg


def _mesh_edge_collapse_thinning(
    V: np.ndarray,
    F: np.ndarray,
    *,
    passes: int = 1,
    length_percentile: float = 0.2,
    preserve_branch_degree: int = 3,
    mode: str = "percentile",
    ratio: float = 0.1,
    protect_weights: np.ndarray | None = None,
    protect_threshold: float | None = None,
    protect_targets: np.ndarray | None = None,
    closest_pole_policy: bool = False,
    closest_pole_tol: float = 1.05,
    _log: Optional[logging.Logger] = None,
) -> tuple[np.ndarray, np.ndarray]:
    """Greedy mesh edge-collapse thinning.

    Two modes:
    - mode="percentile": collapse edges shorter than `length_percentile` threshold per pass.
    - mode="pq": collapse up to `ratio` fraction (0..1) of globally shortest edges per pass (single-shot order).
    - mode="pq_heap": priority queue with stale-entry handling; after each collapse, nearby edge lengths are effectively updated (via re-push), improving local ordering.

    Collapses merge endpoints to midpoints, skipping vertices with degree >= preserve_branch_degree.
    Compacts vertices and drops degenerate faces after each pass.
    """
    if F.size == 0 or V.size == 0:
        return V.copy(), F.copy()

    V_curr = V.copy()
    F_curr = F.copy().astype(int, copy=False)

    for p in range(max(1, passes)):
        nv = V_curr.shape[0]
        edges = _mesh_unique_edges(F_curr)
        if edges.size == 0:
            break
        L = np.linalg.norm(V_curr[edges[:, 0]] - V_curr[edges[:, 1]], axis=1)
        if L.size == 0:
            break
        pct = float(np.clip(length_percentile, 0.0, 1.0))
        thr = float(np.quantile(L, pct))
        if not np.isfinite(thr) or thr <= 0:
            thr = float(np.min(L[L > 0])) if np.any(L > 0) else 0.0

        deg = _vertex_degrees_from_faces(nv, F_curr)
        parent = np.arange(nv, dtype=int)
        alive = np.ones(nv, dtype=bool)

        order = np.argsort(L)
        collapsed = 0
        # Determine stopping rule based on mode
        max_collapses = None
        if mode in ("pq", "pq_heap"):
            r = float(np.clip(ratio, 0.0, 1.0))
            max_collapses = int(np.floor(r * len(order)))

        def find(x: int) -> int:
            while parent[x] != parent[parent[x]]:
                parent[x] = parent[parent[x]]
            return parent[x]

        if mode == "pq_heap":
            # Build adjacency (edge indices per vertex) to re-push affected edges after collapses
            adj: list[list[int]] = [[] for _ in range(nv)]
            for ei, (i, j) in enumerate(edges):
                adj[int(i)].append(ei)
                adj[int(j)].append(ei)
            heap: list[tuple[float, int]] = [(float(L[i]), int(i)) for i in range(len(L))]
            heapq.heapify(heap)

            while heap and (max_collapses is None or collapsed < max_collapses):
                d, ei = heapq.heappop(heap)
                a0, b0 = int(edges[ei, 0]), int(edges[ei, 1])
                a = find(a0)
                b = find(b0)
                if a == b:
                    continue
                # Recompute current length
                dcur = float(np.linalg.norm(V_curr[a] - V_curr[b]))
                # Skip if this entry is stale and no longer minimal for this pair
                if dcur > d * 1.0001:
                    # push updated current value and continue
                    heapq.heappush(heap, (dcur, ei))
                    continue
                # Degree and medial-protect checks
                if deg[a] >= preserve_branch_degree or deg[b] >= preserve_branch_degree:
                    continue
                if protect_weights is not None and protect_threshold is not None:
                    if a < protect_weights.shape[0] and b < protect_weights.shape[0]:
                        if min(protect_weights[a], protect_weights[b]) >= protect_threshold:
                            continue
                if not _edge_link_condition_ok(a, b, F_curr, incident, neighbors):
                    continue
                if _collapse_creates_degeneracy(a, b, F_curr, parent, incident):
                    continue
                # Closest-pole policy: midpoint should not move far from pole targets
                if closest_pole_policy and protect_targets is not None:
                    if a < protect_targets.shape[0] and b < protect_targets.shape[0]:
                        ta = protect_targets[a]
                        tb = protect_targets[b]
                        d_a0 = float(np.linalg.norm(V_curr[a] - ta))
                        d_b0 = float(np.linalg.norm(V_curr[b] - tb))
                        mid = 0.5 * (V_curr[a] + V_curr[b])
                        d_a1 = float(np.linalg.norm(mid - ta))
                        d_b1 = float(np.linalg.norm(mid - tb))
                        tol = float(max(1.0, closest_pole_tol))
                        if (d_a1 > tol * d_a0) and (d_b1 > tol * d_b0):
                            continue
                # Collapse b->a
                V_curr[a] = 0.5 * (V_curr[a] + V_curr[b])
                parent[b] = a
                alive[b] = False
                collapsed += 1
                # After merge, affected edges include those incident to a or b; push updated lengths
                for eidx in adj[a0] + adj[b0]:
                    i, j = int(edges[eidx, 0]), int(edges[eidx, 1])
                    ii, jj = find(i), find(j)
                    if ii == jj:
                        continue
                    heapq.heappush(heap, (float(np.linalg.norm(V_curr[ii] - V_curr[jj])), eidx))
                if max_collapses is not None and collapsed >= max_collapses:
                    break
        else:
            # percentile or simple pq (single-shot order)
            for idx in order:
                a, b = int(edges[idx, 0]), int(edges[idx, 1])
                if mode != "pq" and L[idx] > thr:
                    break
                if parent[a] != a or parent[b] != b:
                    continue
                if deg[a] >= preserve_branch_degree or deg[b] >= preserve_branch_degree:
                    continue
                # Medial protection: if both endpoints strongly protected, skip collapse
                if protect_weights is not None and protect_threshold is not None:
                    if a < protect_weights.shape[0] and b < protect_weights.shape[0]:
                        if min(protect_weights[a], protect_weights[b]) >= protect_threshold:
                            continue
                # Topology safety: link-condition check for manifoldness
                if not _edge_link_condition_ok(a, b, F_curr, incident, neighbors):
                    continue
                # Local validity: ensure collapsing b->a doesn't create degenerate faces
                if _collapse_creates_degeneracy(a, b, F_curr, parent, incident):
                    continue
                V_curr[a] = 0.5 * (V_curr[a] + V_curr[b])
                parent[b] = a
                alive[b] = False
                collapsed += 1
                if max_collapses is not None and collapsed >= max_collapses:
                    break

        if collapsed > 0:
            # path compression
            for i in range(nv):
                while parent[i] != parent[parent[i]]:
                    parent[i] = parent[parent[i]]
            Fm = parent[F_curr]
            keep = (Fm[:, 0] != Fm[:, 1]) & (Fm[:, 1] != Fm[:, 2]) & (Fm[:, 2] != Fm[:, 0])
            Fm = Fm[keep]
            roots = np.unique(parent[alive])
            new_index = -np.ones(nv, dtype=int)
            new_index[roots] = np.arange(len(roots))
            V_curr = V_curr[roots]
            F_curr = new_index[Fm]
            if _log is not None:
                _log.info(
                    "Mesh collapse pass %d: collapsed=%d, nv->%d, nf->%d, thr=%.3g",
                    p + 1,
                    collapsed,
                    V_curr.shape[0],
                    F_curr.shape[0],
                    thr,
                )
        else:
            break

    return V_curr, F_curr


def _vertex_incident_faces(F: np.ndarray, nv: int) -> list[list[int]]:
    inc: list[list[int]] = [[] for _ in range(nv)]
    for fi, tri in enumerate(F):
        inc[int(tri[0])].append(fi)
        inc[int(tri[1])].append(fi)
        inc[int(tri[2])].append(fi)
    return inc


def _vertex_neighbors_from_faces(F: np.ndarray, nv: int) -> list[set[int]]:
    nbrs: list[set[int]] = [set() for _ in range(nv)]
    for tri in F:
        i0, i1, i2 = int(tri[0]), int(tri[1]), int(tri[2])
        nbrs[i0].update((i1, i2))
        nbrs[i1].update((i0, i2))
        nbrs[i2].update((i0, i1))
    return nbrs


def _edge_link_condition_ok(
    a: int,
    b: int,
    F: np.ndarray,
    incident: list[list[int]],
    neighbors: list[set[int]],
) -> bool:
    """Approximate link condition for collapsing edge (a,b) in a manifold triangle mesh.

    For a manifold interior edge, the common neighbors of a and b should equal the set of
    third vertices of faces incident to both a and b, and that set should have size <= 2.
    """
    # Identify third vertices of faces that contain both a and b
    third: set[int] = set()
    faces_a = incident[a]
    faces_b = set(incident[b])
    for fi in faces_a:
        tri = F[fi]
        ia, ib, ic = int(tri[0]), int(tri[1]), int(tri[2])
        vs = {ia, ib, ic}
        if a in vs and b in vs and len(vs) == 3:
            vs.remove(a)
            vs.remove(b)
            c = vs.pop()
            third.add(c)
    # Common neighbors from 1-ring
    common = neighbors[a].intersection(neighbors[b])
    # Allow boundary-like cases where third may be size 1
    if len(third) > 2:
        return False
    # Require that the common 1-ring equals the third set (no extra common neighbors)
    if common != third:
        return False
    return True
def _collapse_creates_degeneracy(
    a: int,
    b: int,
    F: np.ndarray,
    parent: np.ndarray,
    incident: list[list[int]],
) -> bool:
    """Check if collapsing vertex b into a makes any incident face degenerate.

    We simulate the local mapping b->a on faces incident to a or b and check if any
    face ends up with duplicate vertex indices.
    """
    # Build a minimal local map (copy only affected entries)
    # Use parent as base but override b->a
    # Check faces touching a or b
    touched = set(incident[a]) | set(incident[b])
    for fi in touched:
        v0, v1, v2 = int(parent[F[fi, 0]]), int(parent[F[fi, 1]]), int(parent[F[fi, 2]])
        # Apply local collapse b->a
        if v0 == b:
            v0 = a
        if v1 == b:
            v1 = a
        if v2 == b:
            v2 = a
        # Degenerate if any two indices equal
        if v0 == v1 or v1 == v2 or v2 == v0:
            return True
    return False
def _rebuild_knn_graph_from_existing_nodes(G_in: nx.Graph, k: int) -> nx.Graph:
    points = np.array([G_in.nodes[i]["pos"] for i in G_in.nodes])
    G = _knn_graph(points, k=k)
    return G


def _prune_short_leaves(G: nx.Graph, quantile: float = 0.7, min_keep_edges: int = 1) -> None:
    if G.number_of_edges() == 0:
        return
    # Compute threshold
    elens = np.array([d["weight"] for _, _, d in G.edges(data=True)], dtype=float)
    thr = float(np.quantile(elens, quantile)) if len(elens) > 0 else 0.0

    changed = True
    while changed:
        changed = False
        leaves = [n for n in list(G.nodes) if G.degree(n) <= 1]
        if len(G.edges) <= min_keep_edges:
            break
        for n in leaves:
            nbrs = list(G.neighbors(n))
            if not nbrs:
                if G.number_of_nodes() > 1:
                    G.remove_node(n)
                changed = True
                continue
            u = nbrs[0]
            w = G[n][u]["weight"]
            if w < thr and G.number_of_edges() > min_keep_edges:
                G.remove_node(n)
                changed = True


def _graph_priority_edge_collapse(
    G: nx.Graph,
    *,
    preserve_branch_degree: int = 3,
    length_percentile: float = 0.3,
    min_edges_target: int | None = None,
    verbose: bool = False,
    _log: Optional[logging.Logger] = None,
    protect_weights: np.ndarray | None = None,
    protect_threshold: float | None = None,
    mode: str = "percentile",
    ratio: float = 0.1,
    targets: np.ndarray | None = None,
    closest_pole_policy: bool = False,
    closest_pole_tol: float = 1.05,
) -> None:
    """Iteratively collapse the shortest edges in a point-graph while preserving junctions.

    - Never collapse an edge if either endpoint has degree >= preserve_branch_degree
      (heuristic to preserve branch points akin to Skelcollapse's behavior).
    - When collapsing (u,v), create a new node at midpoint and reconnect neighbors.
    - Remove duplicate edges and self-loops; keep minimal weights.

    Modifies G in-place.
    """
    if G.number_of_edges() == 0:
        return
    log = _log or logger

    # Compute all edge lengths
    def edge_length(u: int, v: int) -> float:
        pu = G.nodes[u]["pos"]
        pv = G.nodes[v]["pos"]
        return float(np.linalg.norm(pu - pv))

    edges = list(G.edges())
    lengths = np.array([edge_length(u, v) for (u, v) in edges], dtype=float)
    if len(lengths) == 0:
        return

    # Threshold: collapse very short edges (default below given percentile)
    pct = float(np.clip(length_percentile, 0.0, 1.0))
    thr = float(np.quantile(lengths, pct))
    if thr <= 0:
        thr = float(np.min(lengths[lengths > 0])) if np.any(lengths > 0) else 0.0

    # Priority order by length
    order = np.argsort(lengths)
    collapsed = 0

    # Determine stopping rule based on mode
    max_collapses = None
    if mode == "pq":
        r = float(np.clip(ratio, 0.0, 1.0))
        max_collapses = int(np.floor(r * len(order)))

    for idx in order:
        if min_edges_target is not None and G.number_of_edges() <= min_edges_target:
            break
        (u, v) = edges[idx]
        if u not in G or v not in G or not G.has_edge(u, v):
            continue
        # Skip if long
        d = edge_length(u, v)
        if mode != "pq" and d > thr:
            continue
        # Preserve high-degree junctions
        if G.degree[u] >= preserve_branch_degree or G.degree[v] >= preserve_branch_degree:
            continue
        # Medial protection on nodes
        if protect_weights is not None and protect_threshold is not None:
            # Node order is 0..N-1 after relabeling above
            if u < protect_weights.shape[0] and v < protect_weights.shape[0]:
                if min(protect_weights[u], protect_weights[v]) >= protect_threshold:
                    continue

        # Closest-pole policy: merging midpoint should not move far from mapped pole targets
        if closest_pole_policy and targets is not None and len(targets) > 0:
            if u < targets.shape[0] and v < targets.shape[0]:
                tu = targets[u]
                tv = targets[v]
                pu = G.nodes[u]["pos"]
                pv = G.nodes[v]["pos"]
                mid = 0.5 * (pu + pv)
                du0 = float(np.linalg.norm(pu - tu))
                dv0 = float(np.linalg.norm(pv - tv))
                du1 = float(np.linalg.norm(mid - tu))
                dv1 = float(np.linalg.norm(mid - tv))
                tol = float(max(1.0, closest_pole_tol))
                if (du1 > tol * du0) and (dv1 > tol * dv0):
                    continue

        # Merge u and v into a new node w at midpoint
        pu = G.nodes[u]["pos"]
        pv = G.nodes[v]["pos"]
        wpos = 0.5 * (pu + pv)

        # Create new node id
        w = max(G.nodes) + 1 if G.number_of_nodes() > 0 else 0
        G.add_node(w, pos=wpos)

        # Collect neighbors (excluding u,v)
        nbrs: set[int] = set(G.neighbors(u)) | set(G.neighbors(v))
        nbrs.discard(u)
        nbrs.discard(v)

        # Remove u,v and connect w to neighbors with updated weights
        G.remove_node(u)
        G.remove_node(v)

        for a in nbrs:
            pa = G.nodes[a]["pos"]
            wlen = float(np.linalg.norm(pa - wpos))
            if a == w:
                continue
            if G.has_edge(a, w):
                if wlen < G[a][w]["weight"]:
                    G[a][w]["weight"] = wlen
            else:
                G.add_edge(a, w, weight=wlen)

        collapsed += 1

    if verbose:
        log.info("Edge-collapse: collapsed %d edges below thr=%.3g; nodes=%d, edges=%d", collapsed, thr, G.number_of_nodes(), G.number_of_edges())


def _map_weights_to_points(points: np.ndarray, base_points: np.ndarray, base_weights: np.ndarray) -> np.ndarray:
    """Map scalar weights from base_points onto points via nearest neighbor.

    Returns an array of shape (len(points),) with weights in the range of base_weights.
    """
    if len(points) == 0:
        return np.zeros((0,), dtype=float)
    tree = cKDTree(base_points)
    d, idx = tree.query(points, k=1)
    w = base_weights[idx]
    return np.asarray(w, dtype=float)


def _compress_degree_two_chains(G: nx.Graph) -> nx.Graph:
    """Compress sequences of degree-2 nodes into single edges connecting junctions.

    - Junctions are nodes with degree != 2; degree-1 leaves and degree>=3 junctions remain.
    - Chains between junctions become a single edge with weight equal to the sum of
      intermediate edge weights. Node positions for junctions are preserved.
    - If the entire graph is a single cycle (all degree==2), return G unchanged.
    """
    if G.number_of_nodes() == 0:
        return G
    deg = dict(G.degree())
    junctions = [n for n, d in deg.items() if d != 2]
    if len(junctions) == 0:
        # likely a pure cycle; leave as-is
        return G

    NG = nx.Graph()
    # Copy junction nodes with positions
    for n in junctions:
        NG.add_node(n, pos=np.array(G.nodes[n]["pos"]))

    visited: set[tuple[int, int]] = set()
    for u in junctions:
        for v in G.neighbors(u):
            if (u, v) in visited or (v, u) in visited:
                continue
            path_len = float(G[u][v].get("weight", np.linalg.norm(G.nodes[u]["pos"] - G.nodes[v]["pos"])) )
            prev = u
            curr = v
            visited.add((u, v))
            # Walk forward through degree-2 nodes
            while deg.get(curr, 0) == 2 and curr not in junctions:
                nbrs = list(G.neighbors(curr))
                nxt = nbrs[0] if nbrs[1] == prev else nbrs[1]
                path_len += float(G[curr][nxt].get("weight", np.linalg.norm(G.nodes[curr]["pos"] - G.nodes[nxt]["pos"])) )
                prev, curr = curr, nxt
                visited.add((prev, curr))

            # Now curr is a junction (or leaf)
            a, b = u, curr
            if a == b:
                continue
            w = path_len
            # Ensure nodes exist
            if a not in NG:
                NG.add_node(a, pos=np.array(G.nodes[a]["pos"]))
            if b not in NG:
                NG.add_node(b, pos=np.array(G.nodes[b]["pos"]))
            if NG.has_edge(a, b):
                # keep minimal weight if duplicate, though duplicates should be rare
                if w < NG[a][b]["weight"]:
                    NG[a][b]["weight"] = w
            else:
                NG.add_edge(a, b, weight=w)

    return NG


def _resample_edges_uniform(G_in: nx.Graph, spacing: float) -> nx.Graph:
    """Subdivide edges to have segments approximately equal to spacing.

    Builds a new graph with new node IDs. Original junction node positions are
    preserved; intermediate nodes are inserted along edges at uniform steps.
    """
    if G_in.number_of_edges() == 0:
        return G_in
    NG = nx.Graph()
    # Map original node -> new node id
    node_map: dict[int, int] = {}

    def ensure_node(n: int) -> int:
        if n in node_map:
            return node_map[n]
        nid = NG.number_of_nodes()
        NG.add_node(nid, pos=np.array(G_in.nodes[n]["pos"]))
        node_map[n] = nid
        return nid

    for u, v, data in G_in.edges(data=True):
        p0 = np.array(G_in.nodes[u]["pos"])  # type: ignore[index]
        p1 = np.array(G_in.nodes[v]["pos"])  # type: ignore[index]
        dist = float(np.linalg.norm(p1 - p0))
        if dist <= 0:
            # degenerate, just ensure nodes exist and add zero edge
            a = ensure_node(u)
            b = ensure_node(v)
            if a != b and not NG.has_edge(a, b):
                NG.add_edge(a, b, weight=0.0)
            continue
        segs = max(1, int(np.ceil(dist / spacing)))
        a = ensure_node(u)
        if segs == 1:
            b = ensure_node(v)
            if a != b:
                NG.add_edge(a, b, weight=dist)
            continue
        # Insert intermediate nodes
        step = (p1 - p0) / segs
        prev = a
        for s in range(1, segs):
            pos = p0 + step * s
            nid = NG.number_of_nodes()
            NG.add_node(nid, pos=pos)
            NG.add_edge(prev, nid, weight=float(np.linalg.norm(step)))
            prev = nid
        b = ensure_node(v)
        NG.add_edge(prev, b, weight=float(np.linalg.norm(step)))

    return NG
